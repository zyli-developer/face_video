# 开发计划

## 一、项目目标

实现基于摄像头的人脸表情识别、情感分类、调用本地免费大语言模型生成文字，并将生成的文字内容自动合成为视频，适用于儿童互动、情感陪伴等场景。

---

## 二、项目结构与主框架建议

- **主框架建议**：
  - 推荐使用 Python 作为主开发语言，因其生态丰富，AI/多媒体处理库齐全。
  - 可选用 FastAPI 或 Flask 作为后端服务框架，便于后续接口扩展和前后端分离。
  - 前端可选用 Gradio（快速原型）、Streamlit 或 React（如需自定义UI）。

- **目录结构建议**：
  ```
  face_video/
    ├── main.py                # 项目主入口
    ├── camera/                # 摄像头与表情识别相关代码
    │     └── detector.py
    ├── emotion/               # 情感分类与映射
    │     └── classifier.py
    ├── llm/                   # 大语言模型调用
    │     └── generator.py
    ├── tts/                   # 文字转语音
    │     └── tts_engine.py
    ├── asr/                   # 语音转文字（可选）
    │     └── asr_engine.py
    ├── video/                 # 视频合成相关
    │     └── composer.py
    ├── static/                # 静态资源（图片、音频、视频等）
    ├── requirements.txt       # 依赖包
    └── README.md
  ```

---

## 三、分组件开发与主要功能

### 1. 摄像头与表情识别组件（camera/detector.py）
- **主要功能**：
  - 打开摄像头，实时采集视频帧
  - 检测人脸区域
  - 识别表情类别
- **主要方法**：
  - `open_camera()`：初始化摄像头
  - `get_frame()`：获取当前帧
  - `detect_faces(frame)`：检测人脸
  - `predict_emotion(face_img)`：识别表情类别

### 2. 情感分类组件（emotion/classifier.py）
- **主要功能**：
  - 将表情类别映射为情感标签（如"开心"、"难过"）
  - 支持自定义映射规则
- **主要方法**：
  - `map_emotion_to_sentiment(emotion_label)`：表情到情感的映射
  - `customize_mapping(mapping_dict)`：自定义映射规则

### 3. 大语言模型组件（llm/generator.py）
- **主要功能**：
  - 根据情感标签生成故事或安慰性文字
  - 支持多种本地大模型（ChatGLM、Qwen等）
- **主要方法**：
  - `generate_text(sentiment, prompt_template=None)`：生成文本
  - `load_model(model_name)`：加载本地大模型

### 4. 文字转语音组件（tts/tts_engine.py）
- **主要功能**：
  - 将文本内容转为语音文件
  - 支持多种TTS模型
- **主要方法**：
  - `text_to_speech(text, output_path)`：文本转语音
  - `set_tts_model(model_name)`：切换TTS模型

### 5. 语音转文字组件（可选，asr/asr_engine.py）
- **主要功能**：
  - 将语音音频转为文字
- **主要方法**：
  - `speech_to_text(audio_path)`：语音转文字
  - `set_asr_model(model_name)`：切换ASR模型

### 6. 视频合成组件（video/composer.py）
- **主要功能**：
  - 合成音频、字幕、背景为视频
  - 支持自定义背景、字幕样式
- **主要方法**：
  - `compose_video(audio_path, subtitle, output_path, background_path=None)`：合成视频
  - `add_subtitle_to_video(video_path, subtitle)`：添加字幕
  - `set_video_style(style_dict)`：自定义视频样式

### 7. 主流程调度（main.py）
- **主要功能**：
  - 串联各组件，完成端到端流程
  - 提供命令行或Web接口入口
- **主要方法**：
  - `main()`：主流程入口
  - `run_pipeline()`：自动化处理流程
  - `api_endpoint()`：Web接口（如采用FastAPI/Flask）

---

## 四、团队协作建议
- 每个组件可由不同开发者负责，接口文档提前约定，便于并行开发。
- 采用Git进行版本管理，建议分支开发、定期合并。
- 重要方法和接口需配合单元测试，保证集成稳定。

---

## 五、补充说明
- 组件方法名称可根据实际开发风格微调，但建议保持语义清晰、职责单一。
- 如需前端交互，可单独设立前端目录（如web/），与后端API对接。
- 具体实现时可根据实际需求灵活调整结构。

---

如需进一步细化某一组件的实现细节或接口定义，请随时告知！

---

## 六、开发阶段划分

### 阶段一：基础功能原型
- 摄像头采集与人脸表情识别
- 情感标签分类
- 文本生成（调用本地大语言模型）
- 文字转语音（TTS）
- 语音与字幕合成视频

### 阶段二：功能完善与优化
- 优化表情识别准确率
- 丰富情感分类与Prompt模板
- 支持多种TTS模型和发音风格
- 视频合成美化（自定义背景、动画等）
- 增加异常处理与用户交互体验

### 阶段三：部署与测试
- 跨平台兼容性测试
- 性能优化与资源占用评估
- 用户体验测试与反馈收集
- 文档完善与开源发布

---

## 七、主要任务与关键技术选型

| 任务                | 技术/工具         | 说明 |
|---------------------|------------------|------|
| 摄像头采集与表情识别 | OpenCV + CNN/FER2013/EmoDetect | 实时检测人脸与表情 |
| 情感分类            | 规则映射/自定义分类 | 将表情结果映射为情感标签 |
| 文本生成            | ChatGLM/Qwen/LLaMA2/3 | 本地大语言模型，免费开源 |
| 文字转语音（TTS）   | Coqui TTS         | 免费开源，支持中文 |
| 语音转文字（ASR）   | Whisper/Vosk      | 如需语音识别，可选 |
| 视频合成            | moviepy/ffmpeg    | 合成音频、字幕、背景为视频 |

---

## 八、时间安排建议

| 阶段         | 预计时间 | 主要目标 |
|--------------|----------|----------|
| 阶段一       | 1-2周    | 跑通端到端原型，完成主流程 |
| 阶段二       | 2-3周    | 功能完善、体验优化、美化界面 |
| 阶段三       | 1周      | 测试、优化、文档整理与发布 |

---

## 九、里程碑

1. 完成摄像头表情识别与情感分类原型
2. 成功调用本地大语言模型生成文本
3. 实现文字转语音与视频自动合成
4. 完成界面优化与多情感场景适配
5. 通过多轮测试并发布开源版本

---

## 十、风险与注意事项

- 本地大模型推理需较高算力，建议优先测试轻量模型
- TTS/ASR模型需提前下载权重，注意兼容性
- 视频合成时注意音视频同步与字幕显示效果
- 用户隐私与数据安全需合规处理
- 需持续关注开源依赖的更新与维护

---

如需根据实际进展调整计划，可灵活增减阶段和任务。 