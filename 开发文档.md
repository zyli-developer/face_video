# 人脸表情识别与视频生成系统开发文档

## 1. 项目概述

### 1.1 项目背景
本项目旨在开发一个基于摄像头的人脸表情识别系统，能够实时检测用户表情，根据情感状态调用本地大语言模型生成相应的故事或安慰性文字，并自动合成为带配音和字幕的视频。

### 1.2 项目目标
- 实现实时人脸表情识别
- 根据表情进行情感分类
- 调用本地免费大语言模型生成文本
- 将文本转换为语音
- 合成最终视频

### 1.3 应用场景
- 儿童情感陪伴
- 教育互动应用
- 心理健康辅助
- 娱乐应用

## 2. 技术架构

### 2.1 整体架构
```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   摄像头输入     │───▶│   表情识别模块   │───▶│   情感分类模块   │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                                                        │
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   视频输出       │◀───│   视频合成模块   │◀───│   TTS语音模块   │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                                │                       │
                       ┌─────────────────┐    ┌─────────────────┐
                       │   字幕生成模块   │    │   大语言模型    │
                       └─────────────────┘    └─────────────────┘
```

### 2.2 技术栈
- **开发语言**: Python 3.8+
- **计算机视觉**: OpenCV, TensorFlow/PyTorch
- **深度学习**: CNN (FER2013), 预训练模型
- **大语言模型**: ChatGLM, Qwen, LLaMA2/3
- **语音合成**: Coqui TTS
- **视频处理**: moviepy, ffmpeg
- **Web框架**: FastAPI/Flask (可选)
- **前端**: Gradio/Streamlit (快速原型)

## 3. 详细设计

### 3.1 模块设计

#### 3.1.1 摄像头与表情识别模块 (camera/detector.py)
```python
class EmotionDetector:
    def __init__(self, model_path: str, cascade_path: str):
        """初始化表情识别器"""
        pass
    
    def open_camera(self, camera_id: int = 0) -> bool:
        """打开摄像头"""
        pass
    
    def get_frame(self) -> np.ndarray:
        """获取当前帧"""
        pass
    
    def detect_faces(self, frame: np.ndarray) -> List[Tuple[int, int, int, int]]:
        """检测人脸区域"""
        pass
    
    def predict_emotion(self, face_img: np.ndarray) -> str:
        """识别表情类别"""
        pass
    
    def process_frame(self) -> Tuple[np.ndarray, str]:
        """处理单帧，返回处理后的帧和表情结果"""
        pass
```

#### 3.1.2 情感分类模块 (emotion/classifier.py)
```python
class EmotionClassifier:
    def __init__(self):
        """初始化情感分类器"""
        self.emotion_mapping = {
            'happy': 'positive',
            'sad': 'negative',
            'angry': 'negative',
            'surprise': 'neutral',
            'fear': 'negative',
            'disgust': 'negative',
            'neutral': 'neutral'
        }
    
    def map_emotion_to_sentiment(self, emotion_label: str) -> str:
        """将表情映射为情感标签"""
        pass
    
    def customize_mapping(self, mapping_dict: Dict[str, str]) -> None:
        """自定义映射规则"""
        pass
    
    def get_emotion_intensity(self, emotion_label: str) -> float:
        """获取情感强度"""
        pass
```

#### 3.1.3 大语言模型模块 (llm/generator.py)
```python
class TextGenerator:
    def __init__(self, model_name: str = "chatglm"):
        """初始化文本生成器"""
        pass
    
    def load_model(self, model_name: str) -> bool:
        """加载本地大模型"""
        pass
    
    def generate_text(self, sentiment: str, prompt_template: str = None) -> str:
        """根据情感生成文本"""
        pass
    
    def create_prompt(self, sentiment: str) -> str:
        """创建提示词模板"""
        pass
    
    def post_process_text(self, text: str) -> str:
        """后处理生成的文本"""
        pass
```

#### 3.1.4 TTS模块 (tts/tts_engine.py)
```python
class TTSEngine:
    def __init__(self, model_name: str = "tts_models/zh-CN/baker/tacotron2-DDC-GST"):
        """初始化TTS引擎"""
        pass
    
    def text_to_speech(self, text: str, output_path: str) -> bool:
        """文本转语音"""
        pass
    
    def set_tts_model(self, model_name: str) -> bool:
        """切换TTS模型"""
        pass
    
    def adjust_speech_rate(self, rate: float) -> None:
        """调整语速"""
        pass
    
    def adjust_pitch(self, pitch: float) -> None:
        """调整音调"""
        pass
```

#### 3.1.5 视频合成模块 (video/composer.py)
```python
class VideoComposer:
    def __init__(self):
        """初始化视频合成器"""
        pass
    
    def compose_video(self, audio_path: str, subtitle: str, 
                     output_path: str, background_path: str = None) -> bool:
        """合成视频"""
        pass
    
    def add_subtitle_to_video(self, video_path: str, subtitle: str) -> bool:
        """添加字幕"""
        pass
    
    def set_video_style(self, style_dict: Dict) -> None:
        """设置视频样式"""
        pass
    
    def create_background(self, size: Tuple[int, int], 
                         color: str = "black") -> np.ndarray:
        """创建背景"""
        pass
```

### 3.2 数据流设计

#### 3.2.1 主要数据流
1. **摄像头数据流**: 摄像头 → 帧数据 → 人脸检测 → 表情识别
2. **情感处理流**: 表情结果 → 情感分类 → 大模型生成 → 文本输出
3. **媒体处理流**: 文本 → TTS → 音频 → 视频合成 → 最终视频

#### 3.2.2 数据格式
- **图像数据**: numpy.ndarray (BGR格式)
- **音频数据**: WAV格式 (16kHz, 16bit)
- **视频数据**: MP4格式 (H.264编码)
- **文本数据**: UTF-8编码字符串

## 4. API接口设计

### 4.1 RESTful API (如果使用FastAPI)

#### 4.1.1 表情识别接口
```python
@app.post("/detect_emotion")
async def detect_emotion(image: UploadFile):
    """上传图片进行表情识别"""
    pass

@app.get("/stream_emotion")
async def stream_emotion():
    """实时表情识别流"""
    pass
```

#### 4.1.2 文本生成接口
```python
@app.post("/generate_text")
async def generate_text(sentiment: str, custom_prompt: str = None):
    """根据情感生成文本"""
    pass
```

#### 4.1.3 视频合成接口
```python
@app.post("/compose_video")
async def compose_video(text: str, background: UploadFile = None):
    """合成视频"""
    pass
```

### 4.2 内部接口

#### 4.2.1 模块间通信
```python
class PipelineManager:
    def __init__(self):
        self.detector = EmotionDetector()
        self.classifier = EmotionClassifier()
        self.generator = TextGenerator()
        self.tts_engine = TTSEngine()
        self.composer = VideoComposer()
    
    def run_pipeline(self) -> str:
        """运行完整流程"""
        pass
    
    def process_single_frame(self) -> Dict:
        """处理单帧"""
        pass
```

## 5. 配置管理

### 5.1 配置文件 (config.yaml)
```yaml
# 摄像头配置
camera:
  device_id: 0
  resolution: [640, 480]
  fps: 30

# 模型配置
models:
  emotion_detector: "models/emotion_model.h5"
  face_cascade: "models/haarcascade_frontalface_default.xml"
  llm_model: "chatglm"
  tts_model: "tts_models/zh-CN/baker/tacotron2-DDC-GST"

# 视频配置
video:
  output_format: "mp4"
  resolution: [1280, 720]
  fps: 24
  background_color: "black"

# 情感映射
emotion_mapping:
  happy: "positive"
  sad: "negative"
  angry: "negative"
  surprise: "neutral"
  fear: "negative"
  disgust: "negative"
  neutral: "neutral"
```

## 6. 部署说明

### 6.1 环境要求
- Python 3.8+
- CUDA 11.0+ (如果使用GPU)
- 至少8GB RAM
- 摄像头设备

### 6.2 安装步骤
```bash
# 1. 克隆项目
git clone <repository_url>
cd face_video

# 2. 创建虚拟环境
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. 安装依赖
pip install -r requirements.txt

# 4. 下载模型文件
python scripts/download_models.py

# 5. 运行项目
python main.py
```

### 6.3 Docker部署
```dockerfile
FROM python:3.8-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["python", "main.py"]
```

## 7. 测试计划

### 7.1 单元测试
- 表情识别准确率测试
- 情感分类正确性测试
- 文本生成质量测试
- TTS语音质量测试
- 视频合成功能测试

### 7.2 集成测试
- 端到端流程测试
- 性能压力测试
- 跨平台兼容性测试

### 7.3 用户测试
- 用户体验测试
- 功能完整性测试
- 界面友好性测试

## 8. 性能优化

### 8.1 计算优化
- 使用GPU加速深度学习推理
- 模型量化减少内存占用
- 多线程处理提高并发性能

### 8.2 内存优化
- 及时释放不需要的资源
- 使用生成器处理大数据流
- 优化图像处理算法

### 8.3 存储优化
- 压缩音频和视频文件
- 定期清理临时文件
- 使用缓存减少重复计算

## 9. 安全考虑

### 9.1 隐私保护
- 不保存用户图像数据
- 本地处理，不上传敏感信息
- 遵守数据保护法规

### 9.2 系统安全
- 输入验证和过滤
- 防止恶意文件上传
- 定期更新依赖包

## 10. 维护计划

### 10.1 日常维护
- 监控系统运行状态
- 定期备份重要数据
- 更新模型和依赖

### 10.2 版本更新
- 定期发布新版本
- 向后兼容性保证
- 用户反馈收集和处理

## 11. 扩展计划

### 11.1 功能扩展
- 支持更多表情类别
- 增加语音交互功能
- 支持多语言

### 11.2 技术升级
- 集成更先进的AI模型
- 优化用户体验
- 支持更多平台

---

*本文档将根据项目进展持续更新* 